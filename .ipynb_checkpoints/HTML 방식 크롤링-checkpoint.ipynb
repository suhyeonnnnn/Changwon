{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 그동안 했던 크롤러 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 영화 데이터 리뷰 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.rottentomatoes.com/m/parasite_2019/reviews?type=&sort=&page=1', 'https://www.rottentomatoes.com/m/parasite_2019/reviews?type=&sort=&page=2', 'https://www.rottentomatoes.com/m/parasite_2019/reviews?type=&sort=&page=3', 'https://www.rottentomatoes.com/m/parasite_2019/reviews?type=&sort=&page=4', 'https://www.rottentomatoes.com/m/parasite_2019/reviews?type=&sort=&page=5', 'https://www.rottentomatoes.com/m/parasite_2019/reviews?type=&sort=&page=6', 'https://www.rottentomatoes.com/m/parasite_2019/reviews?type=&sort=&page=7', 'https://www.rottentomatoes.com/m/parasite_2019/reviews?type=&sort=&page=8', 'https://www.rottentomatoes.com/m/parasite_2019/reviews?type=&sort=&page=9', 'https://www.rottentomatoes.com/m/parasite_2019/reviews?type=&sort=&page=10', 'https://www.rottentomatoes.com/m/parasite_2019/reviews?type=&sort=&page=11', 'https://www.rottentomatoes.com/m/parasite_2019/reviews?type=&sort=&page=12', 'https://www.rottentomatoes.com/m/parasite_2019/reviews?type=&sort=&page=13', 'https://www.rottentomatoes.com/m/parasite_2019/reviews?type=&sort=&page=14', 'https://www.rottentomatoes.com/m/parasite_2019/reviews?type=&sort=&page=15', 'https://www.rottentomatoes.com/m/parasite_2019/reviews?type=&sort=&page=16', 'https://www.rottentomatoes.com/m/parasite_2019/reviews?type=&sort=&page=17', 'https://www.rottentomatoes.com/m/parasite_2019/reviews?type=&sort=&page=18', 'https://www.rottentomatoes.com/m/parasite_2019/reviews?type=&sort=&page=19', 'https://www.rottentomatoes.com/m/parasite_2019/reviews?type=&sort=&page=20']\n"
     ]
    }
   ],
   "source": [
    "ran = range(1,21)\n",
    "url = \"https://www.rottentomatoes.com/m/parasite_2019/reviews?type=&sort=&page=\"\n",
    "url_list = []\n",
    "for r in ran:\n",
    "    url_list.append(url+str(r))\n",
    "print(url_list)\n",
    "\n",
    "movie_review = []\n",
    "for url in url_list:        \n",
    "    req = requests.get(url)\n",
    "    html = req.text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    name = soup.select(\n",
    "        'div.review_table > div > div.col-xs-8 > div.col-sm-13.col-xs-24.col-sm-pull-4.critic_name > a.unstyled.bold.articleLink'\n",
    "    )\n",
    "    belongto = soup.select(\n",
    "        'div.review_table > div > div.col-xs-8 > div.col-sm-13.col-xs-24.col-sm-pull-4.critic_name > a > em'\n",
    "    )\n",
    "    review = soup.select(\n",
    "        'div.review_table > div > div.col-xs-16.review_container > div.review_area > div.review_desc > div.the_review'\n",
    "    )\n",
    "    created_at = soup.select(\n",
    "        'div.review_table > div > div.col-xs-16.review_container > div.review_area > div.review-date.subtle.small'\n",
    "    )\n",
    "    \n",
    "    for item in zip(name, belongto, review, created_at):\n",
    "        movie_review.append(\n",
    "            {\n",
    "                'name'      : item[0].text.replace('\\n', '').replace('\\t', '').replace('  ', ''),\n",
    "                'belongto'  : item[1].text.replace('\\n', '').replace('\\t', '').replace('  ', ''),\n",
    "                'review'    : item[2].text.replace('\\n', '').replace('\\t', '').replace('  ', ''),\n",
    "                'created_at': item[3].text.replace('\\n', '').replace('\\t', '').replace('  ', ''),\n",
    "            }\n",
    "        )\n",
    "\n",
    "\n",
    "data = pd.DataFrame(movie_review)\n",
    "data.to_csv('movie_review.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 창원시 데이터 현황 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['http://bigdata.changwon.go.kr/portal/dataset/datasetList.do?groupCodeId=&detailCodeId=&type=title&search=&page=1', 'http://bigdata.changwon.go.kr/portal/dataset/datasetList.do?groupCodeId=&detailCodeId=&type=title&search=&page=2', 'http://bigdata.changwon.go.kr/portal/dataset/datasetList.do?groupCodeId=&detailCodeId=&type=title&search=&page=3', 'http://bigdata.changwon.go.kr/portal/dataset/datasetList.do?groupCodeId=&detailCodeId=&type=title&search=&page=4', 'http://bigdata.changwon.go.kr/portal/dataset/datasetList.do?groupCodeId=&detailCodeId=&type=title&search=&page=5', 'http://bigdata.changwon.go.kr/portal/dataset/datasetList.do?groupCodeId=&detailCodeId=&type=title&search=&page=6', 'http://bigdata.changwon.go.kr/portal/dataset/datasetList.do?groupCodeId=&detailCodeId=&type=title&search=&page=7', 'http://bigdata.changwon.go.kr/portal/dataset/datasetList.do?groupCodeId=&detailCodeId=&type=title&search=&page=8', 'http://bigdata.changwon.go.kr/portal/dataset/datasetList.do?groupCodeId=&detailCodeId=&type=title&search=&page=9', 'http://bigdata.changwon.go.kr/portal/dataset/datasetList.do?groupCodeId=&detailCodeId=&type=title&search=&page=10', 'http://bigdata.changwon.go.kr/portal/dataset/datasetList.do?groupCodeId=&detailCodeId=&type=title&search=&page=11', 'http://bigdata.changwon.go.kr/portal/dataset/datasetList.do?groupCodeId=&detailCodeId=&type=title&search=&page=12', 'http://bigdata.changwon.go.kr/portal/dataset/datasetList.do?groupCodeId=&detailCodeId=&type=title&search=&page=13', 'http://bigdata.changwon.go.kr/portal/dataset/datasetList.do?groupCodeId=&detailCodeId=&type=title&search=&page=14', 'http://bigdata.changwon.go.kr/portal/dataset/datasetList.do?groupCodeId=&detailCodeId=&type=title&search=&page=15', 'http://bigdata.changwon.go.kr/portal/dataset/datasetList.do?groupCodeId=&detailCodeId=&type=title&search=&page=16', 'http://bigdata.changwon.go.kr/portal/dataset/datasetList.do?groupCodeId=&detailCodeId=&type=title&search=&page=17', 'http://bigdata.changwon.go.kr/portal/dataset/datasetList.do?groupCodeId=&detailCodeId=&type=title&search=&page=18']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ran = range(1,19)\n",
    "url = \"http://bigdata.changwon.go.kr/portal/dataset/datasetList.do?groupCodeId=&detailCodeId=&type=title&search=&page=\"\n",
    "url_list = []\n",
    "for r in ran:\n",
    "    url_list.append(url+str(r))\n",
    "print(url_list)\n",
    "\n",
    "datalist = []\n",
    "for url in url_list:        \n",
    "    req = requests.get(url)\n",
    "    html = req.content\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    name = soup.select(\n",
    "        'div.datasetList > ul > li > div > div.datasetListCont > dl.cont > dt > a'\n",
    "    )\n",
    "    category = soup.select(\n",
    "        'div.datasetList > ul > li > div > div.datasetListCont > dl.cont > dd > span'\n",
    "    )\n",
    "   \n",
    "    \n",
    "    for item in zip(name, category):\n",
    "        datalist.append(\n",
    "            {\n",
    "                'name'      : item[0].text.replace('\\n', '').replace('\\t', '').replace('  ', ''),\n",
    "                'category'  : item[1].text.replace('\\n', '').replace('\\t', '').replace('  ', ''),\n",
    "                \n",
    "            }\n",
    "        )\n",
    "\n",
    "\n",
    "data = pd.DataFrame(datalist)\n",
    "data.to_csv('journallist.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AMA 저널 리스트 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-fcb79392db07>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-fcb79392db07>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    제목: #frmSearchResults > div > ol > li:nth-child(1) > article > h2 > span > a\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "제목: #frmSearchResults > div > ol > li:nth-child(1) > article > h2 > span > a\n",
    "저자: #frmSearchResults > div > ol > li:nth-child(1) > article > div.author > div > span > span:nth-child(1) > a\n",
    "저널: #frmSearchResults > div > ol > li:nth-child(1) > article > span.journal-title\n",
    "권호: #frmSearchResults > div > ol > li:nth-child(1) > article > span.issue-meta-volume-issue\n",
    "투고날짜: #frmSearchResults > div > ol > li:nth-child(1) > article > span.publication-meta > span:nth-child(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://journals.sagepub.com/action/doSearch?AllField=topic+modeling&SeriesKey=jmxa&content=articlesChapters&sortBy=cited&target=default&queryID=57%2F762853637&testParam=testParam&AfterYear=2017&BeforeYear=2021&pageSize=20&startPage=0', 'https://journals.sagepub.com/action/doSearch?AllField=topic+modeling&SeriesKey=jmxa&content=articlesChapters&sortBy=cited&target=default&queryID=57%2F762853637&testParam=testParam&AfterYear=2017&BeforeYear=2021&pageSize=20&startPage=1', 'https://journals.sagepub.com/action/doSearch?AllField=topic+modeling&SeriesKey=jmxa&content=articlesChapters&sortBy=cited&target=default&queryID=57%2F762853637&testParam=testParam&AfterYear=2017&BeforeYear=2021&pageSize=20&startPage=2', 'https://journals.sagepub.com/action/doSearch?AllField=topic+modeling&SeriesKey=jmxa&content=articlesChapters&sortBy=cited&target=default&queryID=57%2F762853637&testParam=testParam&AfterYear=2017&BeforeYear=2021&pageSize=20&startPage=3', 'https://journals.sagepub.com/action/doSearch?AllField=topic+modeling&SeriesKey=jmxa&content=articlesChapters&sortBy=cited&target=default&queryID=57%2F762853637&testParam=testParam&AfterYear=2017&BeforeYear=2021&pageSize=20&startPage=4', 'https://journals.sagepub.com/action/doSearch?AllField=topic+modeling&SeriesKey=jmxa&content=articlesChapters&sortBy=cited&target=default&queryID=57%2F762853637&testParam=testParam&AfterYear=2017&BeforeYear=2021&pageSize=20&startPage=5', 'https://journals.sagepub.com/action/doSearch?AllField=topic+modeling&SeriesKey=jmxa&content=articlesChapters&sortBy=cited&target=default&queryID=57%2F762853637&testParam=testParam&AfterYear=2017&BeforeYear=2021&pageSize=20&startPage=6', 'https://journals.sagepub.com/action/doSearch?AllField=topic+modeling&SeriesKey=jmxa&content=articlesChapters&sortBy=cited&target=default&queryID=57%2F762853637&testParam=testParam&AfterYear=2017&BeforeYear=2021&pageSize=20&startPage=7']\n"
     ]
    }
   ],
   "source": [
    "ran = range(0,8)\n",
    "url = \"https://journals.sagepub.com/action/doSearch?AllField=topic+modeling&SeriesKey=jmxa&content=articlesChapters&sortBy=cited&target=default&queryID=57%2F762853637&testParam=testParam&AfterYear=2017&BeforeYear=2021&pageSize=20&startPage=\"\n",
    "url_list = []\n",
    "for r in ran:\n",
    "    url_list.append(url+str(r))\n",
    "print(url_list)\n",
    "\n",
    "datalist = []\n",
    "for url in url_list:        \n",
    "    req = requests.get(url)\n",
    "    html = req.text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    name = soup.select(\n",
    "        'div > ol > li > article > h2 > span > a'\n",
    "    )\n",
    "    author = soup.select(\n",
    "        'div > ol > li > article > div.author > div > span > span > a'  \n",
    "    )\n",
    "    \n",
    "    journal = soup.select(\n",
    "        'div > ol > li > article > span.journal-title'  \n",
    "    )\n",
    "    \n",
    "    volume = soup.select(\n",
    "        'div > ol > li > article > span.issue-meta-volume-issue'  \n",
    "    )\n",
    "    \n",
    "    date = soup.select(\n",
    "        'div > ol > li > article > span.publication-meta > span'  \n",
    "    )\n",
    "    \n",
    "    for item in zip(name, author, journal, volume, date):\n",
    "        datalist.append(\n",
    "            {\n",
    "                'name'      : item[0].text.replace('\\n', '').replace('\\t', '').replace('  ', ''),\n",
    "                'author'  : item[1].text.replace('\\n', '').replace('\\t', '').replace('  ', ''),\n",
    "                'journal'  : item[2].text.replace('\\n', '').replace('\\t', '').replace('  ', ''),\n",
    "                'volume'  : item[3].text.replace('\\n', '').replace('\\t', '').replace('  ', ''),\n",
    "                'date'  : item[4].text.replace('\\n', '').replace('\\t', '').replace('  ', ''),\n",
    "                \n",
    "                \n",
    "            }\n",
    "        )\n",
    "\n",
    "\n",
    "data = pd.DataFrame(datalist)\n",
    "data.to_csv('journallist.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://journals.sagepub.com/action/doSearch?AllField=topic+model&SeriesKey=mrja&content=articlesChapters&target=default&queryID=60%2F762854340&testParam=testParam&AfterYear=2017&BeforeYear=2021&pageSize=20&startPage=0', 'https://journals.sagepub.com/action/doSearch?AllField=topic+model&SeriesKey=mrja&content=articlesChapters&target=default&queryID=60%2F762854340&testParam=testParam&AfterYear=2017&BeforeYear=2021&pageSize=20&startPage=1', 'https://journals.sagepub.com/action/doSearch?AllField=topic+model&SeriesKey=mrja&content=articlesChapters&target=default&queryID=60%2F762854340&testParam=testParam&AfterYear=2017&BeforeYear=2021&pageSize=20&startPage=2', 'https://journals.sagepub.com/action/doSearch?AllField=topic+model&SeriesKey=mrja&content=articlesChapters&target=default&queryID=60%2F762854340&testParam=testParam&AfterYear=2017&BeforeYear=2021&pageSize=20&startPage=3', 'https://journals.sagepub.com/action/doSearch?AllField=topic+model&SeriesKey=mrja&content=articlesChapters&target=default&queryID=60%2F762854340&testParam=testParam&AfterYear=2017&BeforeYear=2021&pageSize=20&startPage=4', 'https://journals.sagepub.com/action/doSearch?AllField=topic+model&SeriesKey=mrja&content=articlesChapters&target=default&queryID=60%2F762854340&testParam=testParam&AfterYear=2017&BeforeYear=2021&pageSize=20&startPage=5', 'https://journals.sagepub.com/action/doSearch?AllField=topic+model&SeriesKey=mrja&content=articlesChapters&target=default&queryID=60%2F762854340&testParam=testParam&AfterYear=2017&BeforeYear=2021&pageSize=20&startPage=6', 'https://journals.sagepub.com/action/doSearch?AllField=topic+model&SeriesKey=mrja&content=articlesChapters&target=default&queryID=60%2F762854340&testParam=testParam&AfterYear=2017&BeforeYear=2021&pageSize=20&startPage=7', 'https://journals.sagepub.com/action/doSearch?AllField=topic+model&SeriesKey=mrja&content=articlesChapters&target=default&queryID=60%2F762854340&testParam=testParam&AfterYear=2017&BeforeYear=2021&pageSize=20&startPage=8']\n"
     ]
    }
   ],
   "source": [
    "#jmr\n",
    "ran = range(0,9)\n",
    "url = \"https://journals.sagepub.com/action/doSearch?AllField=topic+model&SeriesKey=mrja&content=articlesChapters&target=default&queryID=60%2F762854340&testParam=testParam&AfterYear=2017&BeforeYear=2021&pageSize=20&startPage=\"\n",
    "url_list = []\n",
    "for r in ran:\n",
    "    url_list.append(url+str(r))\n",
    "print(url_list)\n",
    "\n",
    "datalist = []\n",
    "for url in url_list:        \n",
    "    req = requests.get(url)\n",
    "    html = req.text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    name = soup.select(\n",
    "        'div > ol > li > article > h2 > span > a'\n",
    "    )\n",
    "    author = soup.select(\n",
    "        'div > ol > li > article > div.author > div > span > span > a'  \n",
    "    )\n",
    "    \n",
    "    journal = soup.select(\n",
    "        'div > ol > li > article > span.journal-title'  \n",
    "    )\n",
    "    \n",
    "    volume = soup.select(\n",
    "        'div > ol > li > article > span.issue-meta-volume-issue'  \n",
    "    )\n",
    "    \n",
    "    date = soup.select(\n",
    "        'div > ol > li > article > span.publication-meta > span'  \n",
    "    )\n",
    "    \n",
    "    \n",
    "    for item in zip(name, author, journal, volume, date):\n",
    "        datalist.append(\n",
    "            {\n",
    "                'name'      : item[0].text.replace('\\n', '').replace('\\t', '').replace('  ', ''),\n",
    "                'author'  : item[1].text.replace('\\n', '').replace('\\t', '').replace('  ', ''),\n",
    "                'journal'  : item[2].text.replace('\\n', '').replace('\\t', '').replace('  ', ''),\n",
    "                'volume'  : item[3].text.replace('\\n', '').replace('\\t', '').replace('  ', ''),\n",
    "                'date'  : item[4].text.replace('\\n', '').replace('\\t', '').replace('  ', ''),\n",
    "                \n",
    "                \n",
    "            }\n",
    "        )\n",
    "\n",
    "\n",
    "data = pd.DataFrame(datalist)\n",
    "data.to_csv('journallist3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#jar\n",
    "ran = range(1,6)\n",
    "url = \"http://www.journalofadvertisingresearch.com/search/topic%252Bmodel?page=1&facet%5Bpublication-date%5D%5B0%5D=2017-2020\"\n",
    "url_list = []\n",
    "for r in ran:\n",
    "    url_list.append(url+str(r))\n",
    "print(url_list)\n",
    "\n",
    "datalist = []\n",
    "for url in url_list:        \n",
    "    req = requests.get(url)\n",
    "    html = req.text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    name = soup.select(\n",
    "        'div > ol > li > article > h2 > span > a'\n",
    "    )\n",
    "    author = soup.select(\n",
    "        'div > ol > li > article > div.author > div > span > span > a'  \n",
    "    )\n",
    "    \n",
    "    journal = soup.select(\n",
    "        'div > ol > li > article > span.journal-title'  \n",
    "    )\n",
    "    \n",
    "    volume = soup.select(\n",
    "        'div > ol > li > article > span.issue-meta-volume-issue'  \n",
    "    )\n",
    "    \n",
    "    date = soup.select(\n",
    "        'div > ol > li > article > span.publication-meta > span'  \n",
    "    )\n",
    "    \n",
    "    \n",
    "    for item in zip(name, author, journal, volume, date):\n",
    "        datalist.append(\n",
    "            {\n",
    "                'name'      : item[0].text.replace('\\n', '').replace('\\t', '').replace('  ', ''),\n",
    "                'author'  : item[1].text.replace('\\n', '').replace('\\t', '').replace('  ', ''),\n",
    "                'journal'  : item[2].text.replace('\\n', '').replace('\\t', '').replace('  ', ''),\n",
    "                'volume'  : item[3].text.replace('\\n', '').replace('\\t', '').replace('  ', ''),\n",
    "                'date'  : item[4].text.replace('\\n', '').replace('\\t', '').replace('  ', ''),\n",
    "                \n",
    "                \n",
    "            }\n",
    "        )\n",
    "\n",
    "\n",
    "data = pd.DataFrame(datalist)\n",
    "data.to_csv('journallist3.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
